{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks_cwt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import iqr\n",
    "from pyentrp import entropy as ent\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_dir = \"test3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features for accelerometer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = 10#int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = 5#int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "#Helper Functions\n",
    "\n",
    "def Signal_magnitude_area(x,y,z):\n",
    "    \n",
    "    sum = 0    \n",
    "    for i in range(len(x)):\n",
    "        sum += (abs(x[i]) + abs(y[i]) + abs(z[i]))\n",
    "        \n",
    "    return float(sum)/len(x)\n",
    "\n",
    "\n",
    "def Power(x):\n",
    "    \n",
    "    power = (LA.norm(x)**2)/ len(x)\n",
    "    return power\n",
    "    \n",
    "def number_of_peaks(window):\n",
    "    indexes = find_peaks_cwt(window, np.arange(1, len(window)))\n",
    "\n",
    "    return len(indexes)\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def load_files(indir,pickles_indir):\n",
    "    dfs_list = []\n",
    "    features = []\n",
    "    labels = []\n",
    "    bypass = True\n",
    "    pickle_file = Path(pickles_indir+\"/img_accelerometer_features.pickle\")\n",
    "\n",
    "    if pickle_file.exists() and not bypass:\n",
    "        print(\"Found pickle files for accelerometer\")\n",
    "        features = pickle.load(open(pickles_indir+\"/img_accelerometer_features.pickle\", \"rb\"))\n",
    "        labels = pickle.load(open(pickles_indir+\"/img_accelerometer_labels.pickle\", \"rb\"))\n",
    "        dfs_list = pickle.load(open(pickles_indir+\"/img_accelerometer_dfs_list.pickle\", \"rb\"))\n",
    "\n",
    "    else:\n",
    "\n",
    "        for root, dirs, files in os.walk(in_dir):\n",
    "            path = root.split(os.sep)\n",
    "            for f in files:\n",
    "                print(\"/\".join(path) + \"/\" + f)\n",
    "                full_path = \"/\".join(path) + \"/\" + f\n",
    "                if \"gyroscope\" in full_path:\n",
    "                    print(\"Skip \", full_path)\n",
    "                    continue\n",
    "                df = pd.read_csv(full_path, header=None)\n",
    "\n",
    "                print(\"Before trimming: \", df.shape)\n",
    "\n",
    "                df = trim_first_last_n_seconds(df, trim_num_seconds, acc_freq)\n",
    "                if df is None:\n",
    "                    print(\"Continuing\")\n",
    "                    continue\n",
    "\n",
    "                print(\"After trimming: \", df.shape)\n",
    "\n",
    "                #Sample the data according to the size of the window with 50% overlap\n",
    "                for index in range(0, df.shape[0]-window_size, window_jump_steps):\n",
    "                    indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                    window = df.iloc[indexes,:]\n",
    "\n",
    "                    X_list = window[1].tolist()\n",
    "                    Y_list = window[2].tolist()\n",
    "                    Z_list = window[3].tolist()\n",
    "\n",
    "\n",
    "                    #Generate the features for this window\n",
    "\n",
    "\n",
    "           # ****************** Time-Domain Features ************************* #\n",
    "\n",
    "                    #Mean of the signals\n",
    "                    mean_x = np.mean(X_list)\n",
    "                    mean_y = np.mean(Y_list)\n",
    "                    mean_z = np.mean(Z_list)\n",
    "\n",
    "                    #Variance of the signals\n",
    "                    var_x = np.var(X_list)\n",
    "                    var_y = np.var(Y_list)\n",
    "                    var_z = np.var(Z_list)\n",
    "\n",
    "                    #Number of peaks in the signals\n",
    "                    #num_peaks_x = number_of_peaks(X_list)\n",
    "                    #num_peaks_y = number_of_peaks(Y_list)\n",
    "                    #num_peaks_z = number_of_peaks(Z_list)            \n",
    "\n",
    "                    #Median of the signals\n",
    "                    median_x = np.ma.median(X_list)\n",
    "                    median_y = np.ma.median(Y_list)\n",
    "                    median_z = np.ma.median(Z_list)\n",
    "\n",
    "                    #Standard Deviation of the signals\n",
    "                    std_x = np.std(X_list)\n",
    "                    std_y = np.std(Y_list)\n",
    "                    std_z = np.std(Z_list)\n",
    "\n",
    "                    #Compute Signal Magnitude Area\n",
    "                    signal_mag_area = Signal_magnitude_area(X_list, Y_list, Z_list)\n",
    "\n",
    "                    #Maximum and Minimum values and their indexes\n",
    "                    max_x = max(X_list)\n",
    "                    max_index_x = X_list.index(max_x)               \n",
    "                    min_x = min(X_list)\n",
    "                    min_index_x = X_list.index(min_x)\n",
    "\n",
    "                    max_y = max(Y_list)\n",
    "                    max_index_y = Y_list.index(max_y)              \n",
    "                    min_y = min(Y_list)\n",
    "                    min_index_y = Y_list.index(min_y)               \n",
    "\n",
    "                    max_z = max(Z_list)\n",
    "                    max_index_z = Z_list.index(max_z)             \n",
    "                    min_z = min(Z_list)\n",
    "                    min_index_z = Z_list.index(min_z)\n",
    "\n",
    "\n",
    "                    #Power of X,Y and Z signals             \n",
    "                    power_x = Power(X_list)\n",
    "                    power_y = Power(Y_list)\n",
    "                    power_z = Power(Z_list)\n",
    "\n",
    "\n",
    "                    #Skewness and Kurtosis\n",
    "                    skew_x = skew(X_list)\n",
    "                    skew_y = skew(Y_list)\n",
    "                    skew_z = skew(Z_list)\n",
    "\n",
    "                    kurtosis_x = kurtosis(X_list)                \n",
    "                    kurtosis_y = kurtosis(Y_list)\n",
    "                    kurtosis_z = kurtosis(Z_list)\n",
    "\n",
    "\n",
    "                    #Entropy of the signals (Can experiment with different types of Entropy)\n",
    "                    entropy_x = ent.shannon_entropy(X_list)\n",
    "                    entropy_y = ent.shannon_entropy(Y_list)\n",
    "                    entropy_z = ent.shannon_entropy(Z_list)\n",
    "\n",
    "\n",
    "                    #Interquartile range of the signals\n",
    "                    iqr_x = iqr(X_list)\n",
    "                    iqr_y = iqr(Y_list)\n",
    "                    iqr_z = iqr(Z_list)\n",
    "\n",
    "\n",
    "        # ****************** Frequency-Domain Features ************************* #\n",
    "\n",
    "                    #Normalized FFT coefficients\n",
    "                    fft_x = LA.norm(np.fft.rfft(X_list))              \n",
    "                    fft_y = LA.norm(np.fft.rfft(Y_list))   \n",
    "                    fft_z = LA.norm(np.fft.rfft(Z_list))  \n",
    "\n",
    "\n",
    "                    #Store the features\n",
    "                    window_feature = []\n",
    "                    window_feature.append(mean_x)\n",
    "                    window_feature.append(mean_y)\n",
    "                    window_feature.append(mean_z)\n",
    "\n",
    "                    window_feature.append(var_x)\n",
    "                    window_feature.append(var_y)\n",
    "                    window_feature.append(var_z)\n",
    "\n",
    "                    window_feature.append(median_x)\n",
    "                    window_feature.append(median_y)\n",
    "                    window_feature.append(median_z)\n",
    "\n",
    "                    window_feature.append(std_x)\n",
    "                    window_feature.append(std_y)\n",
    "                    window_feature.append(std_z)\n",
    "\n",
    "\n",
    "                    window_feature.append(signal_mag_area)\n",
    "\n",
    "                    window_feature.append(max_x)\n",
    "                    window_feature.append(max_index_x)\n",
    "                    window_feature.append(min_x)\n",
    "                    window_feature.append(min_index_x)\n",
    "\n",
    "                    window_feature.append(max_y)\n",
    "                    window_feature.append(max_index_y)\n",
    "                    window_feature.append(min_y)\n",
    "                    window_feature.append(min_index_y)\n",
    "\n",
    "                    window_feature.append(max_z)\n",
    "                    window_feature.append(max_index_z)\n",
    "                    window_feature.append(min_z)\n",
    "                    window_feature.append(min_index_z)\n",
    "\n",
    "                    window_feature.append(power_x)\n",
    "                    window_feature.append(power_y)\n",
    "                    window_feature.append(power_z)\n",
    "\n",
    "                    window_feature.append(skew_x)\n",
    "                    window_feature.append(kurtosis_x) \n",
    "\n",
    "                    window_feature.append(skew_y)\n",
    "                    window_feature.append(kurtosis_y) \n",
    "\n",
    "                    window_feature.append(skew_z)\n",
    "                    window_feature.append(kurtosis_z) \n",
    "\n",
    "                    window_feature.append(entropy_x)\n",
    "                    window_feature.append(entropy_y)\n",
    "                    window_feature.append(entropy_z)\n",
    "\n",
    "                    window_feature.append(iqr_x)\n",
    "                    window_feature.append(iqr_y)\n",
    "                    window_feature.append(iqr_z)\n",
    "\n",
    "                    window_feature.append(fft_x)\n",
    "                    window_feature.append(fft_y)\n",
    "                    window_feature.append(fft_z)\n",
    "\n",
    "\n",
    "                    #window_feature.append(num_peaks_x)\n",
    "                    #window_feature.append(num_peaks_y)\n",
    "                    #window_feature.append(num_peaks_z)\n",
    "\n",
    "                    #scale = preprocessing.minmax_scale(data, feature_range=(-0.5, 0.5))\n",
    "\n",
    "                    features.append(window_feature)\n",
    "\n",
    "                    labels.append(df[5].iloc[1])\n",
    "                dfs_list.append(df)\n",
    "        dfs = pd.concat(dfs_list)\n",
    "\n",
    "        #pickle.dump(features, open(\"pickles/img_accelerometer_features.pickle\", \"wb\"), protocol=2)\n",
    "        #pickle.dump(labels, open(\"pickles/img_accelerometer_labels.pickle\", \"wb\"), protocol=2)\n",
    "        #pickle.dump(dfs_list, open(\"pickles/img_accelerometer_dfs_list.pickle\", \"wb\"), protocol=2)\n",
    "\n",
    "    features = np.asarray(features)\n",
    "    labels = np.asarray(labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading test and train features from appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183508, 43)\n",
      "(1183508,)\n"
     ]
    }
   ],
   "source": [
    "features_train, labels_train = load_files(indir='data-3',pickles_indir='img_pickles_train')\n",
    "features_test, labels_test = load_files(indir='test3',pickles_indir='img_pickles_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Using LDA to reduce the fimensions into 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda = LinearDiscriminantAnalysis(n_components=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_train = lda.fit(features_train, labels).transform(features_train)\n",
    "features_test = lda.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1183508, 3)\n",
      "(1183508,)\n"
     ]
    }
   ],
   "source": [
    "print(features_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(features_test.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put the train and test features into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df_train = pd.DataFrame(features_train,columns=['a','b','c'])\n",
    "labels_df_train = pd.DataFrame(labels_train,columns=['l'])\n",
    "features_df_test = pd.DataFrame(features_test,columns=['a','b','c'])\n",
    "labels_df_test = pd.DataFrame(labels_test,columns=['l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_df_train['l'] = labels_df_train['l']\n",
    "features_df_test['l'] = labels_df_test['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activities = ['walking','sitting','standing','laying_down']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findind the minimum and maximum using 1 stddev away from mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('axis', 'a')\n",
      "(5.772392346708159e-12, 1.9192292699777684)\n",
      "(-31.621895458060305, 10.54334094324393)\n",
      "('axis', 'b')\n",
      "(-7.767921600725771e-13, 1.2881504023883432)\n",
      "(-32.04581690334954, 14.016145053606209)\n",
      "('axis', 'c')\n",
      "(1.608658157529988e-13, 1.146049195629901)\n",
      "(-38.52436291103095, 8.237474300467811)\n"
     ]
    }
   ],
   "source": [
    "max_=[]\n",
    "min_=[]\n",
    "for axis in ['a','b','c']:\n",
    "    print('axis',axis)\n",
    "    mean =np.mean(features_df_train[axis])\n",
    "    std =np.std(features_df_train[axis]) \n",
    "    print(mean,std)\n",
    "    print(np.min(features_df_train[axis]),np.max(features_df_train[axis]))\n",
    "    max_.append(mean+2*std)\n",
    "    min_.append(mean-2*std)\n",
    "    #max_.append()        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.54334094324393, 14.016145053606209, 8.237474300467811]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-31.621895458060305, -32.04581690334954, -38.52436291103095]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to scale features from feature values to colrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_color(i,num):\n",
    "    scale_ = 255 /(max_[i]-min_[i])\n",
    "    num = max(min_[i],num)\n",
    "    num = min(num,max_[i])\n",
    "    return (num - min_[i])*scale_\n",
    "def get_rgb(feature):\n",
    "    #print(feature)\n",
    "    feature = list(feature)\n",
    "    #print(feature)\n",
    "    col = []\n",
    "    for i in range(0,3):\n",
    "        col.append(int(get_color(i,feature[i])))\n",
    "    #col[1] = 0\n",
    "    return col\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spiral(lst,n):\n",
    "    dx,dy = 1,0            # Starting increments\n",
    "    x,y = 0,0              # Starting location\n",
    "    myarray = [[None]* n for j in range(n)]\n",
    "    for i in range(n**2):\n",
    "        myarray[x][y] = lst[i]\n",
    "        nx,ny = x+dx, y+dy\n",
    "        if 0<=nx<n and 0<=ny<n and myarray[nx][ny] == None:\n",
    "            x,y = nx,ny\n",
    "        else:\n",
    "            dx,dy = -dy,dx\n",
    "            x,y = x+dx, y+dy\n",
    "    return myarray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features converted to 0 - 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_map_train={}\n",
    "for activity in activities:\n",
    "    print('train')\n",
    "    df = features_df_train[features_df_train['l']==activity][['a','b','c']].copy()\n",
    "    print(df.shape)\n",
    "    array_map_train[activity] = (df.as_matrix())\n",
    "    print((array_map_train[activity][2]))\n",
    "    print('test')\n",
    "    df = features_df_test[features_df_test['l']==activity][['a','b','c']].copy()\n",
    "    print(df.shape)\n",
    "    array_map_test[activity] = (df.as_matrix())\n",
    "    print((array_map_test[activity][2]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 255, 37]\n",
      "[54, 122, 166]\n",
      "[200, 80, 65]\n",
      "[187, 79, 205]\n"
     ]
    }
   ],
   "source": [
    "color_map_train = {}\n",
    "color_map_test = {}\n",
    "for activity in activities:\n",
    "    color_map_train[activity] = [get_rgb(x) for x in array_map_train[activity]]\n",
    "    print(color_map_train[activity][1])\n",
    "    color_map_test[activity] = [get_rgb(x) for x in array_map_test[activity]]\n",
    "    print(color_map_test[activity][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverting to square images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "color_map_spiral_train = defaultdict(list)\n",
    "color_map_spiral_test = defaultdict(list)\n",
    "for activity in activities:\n",
    "    print(activity)\n",
    "    siz = len(color_map_train[activity])\n",
    "    for i in range((siz/625)-1):\n",
    "        sp = spiral(color_map_train[activity][i*625:(i+1)*625],25)\n",
    "        color_map_spiral_train[activity].append(sp)\n",
    "    print(len(color_map_spiral_train[activity]))\n",
    "    siz = len(color_map_test[activity])\n",
    "    for i in range((siz/625)-1):\n",
    "        sp = spiral(color_map_test[activity][i*625:(i+1)*625],25)\n",
    "        color_map_spiral_test[activity].append(sp)\n",
    "    print(len(color_map_spiral_test[activity]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laying_down\n",
      "sitting\n",
      "standing\n",
      "walking\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'img_train'\n",
    "test_dir = 'img_test'\n",
    "if not os.path.exists(train_dir):\n",
    "    os.makedirs(train_dir)\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "for activity in activities:\n",
    "    i=0\n",
    "    print(activity)\n",
    "    if not os.path.exists(train_dir+'/'+activity):\n",
    "        os.mkdir(train_dir+'/'+activity)        \n",
    "    if not os.path.exists(test_dir+'/'+activity):\n",
    "        os.mkdir(test_dir+'/'+activity)\n",
    "        \n",
    "    for spir_arr in color_map_spiral_train[activity]:\n",
    "        i+=1\n",
    "        img = Image.fromarray(np.array(spir_arr), 'RGB')\n",
    "        img.save(train_dir+'/'+activity+'/'+activity+str(i)+'.png')\n",
    "    \n",
    "    for spir_arr in color_map_spiral_test[activity]:\n",
    "        i+=1\n",
    "        img = Image.fromarray(np.array(spir_arr), 'RGB')\n",
    "        img.save(test_dir+'/'+activity+'/'+activity+str(i)+'.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1711 images belonging to 4 classes.\n",
      "Found 178 images belonging to 4 classes.\n",
      "Epoch 1/50\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 1.2303 - acc: 0.4083 - val_loss: 0.9308 - val_acc: 0.5852\n",
      "Epoch 2/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.9811 - acc: 0.5575 - val_loss: 0.7358 - val_acc: 0.6307\n",
      "Epoch 3/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8743 - acc: 0.6189 - val_loss: 0.7507 - val_acc: 0.6364\n",
      "Epoch 4/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.8098 - acc: 0.6725 - val_loss: 0.5440 - val_acc: 0.7955\n",
      "Epoch 5/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.7610 - acc: 0.7032 - val_loss: 0.4706 - val_acc: 0.8125\n",
      "Epoch 6/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.7289 - acc: 0.7309 - val_loss: 0.4222 - val_acc: 0.8352\n",
      "Epoch 7/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6928 - acc: 0.7446 - val_loss: 0.4249 - val_acc: 0.8409\n",
      "Epoch 8/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6710 - acc: 0.7540 - val_loss: 0.4081 - val_acc: 0.8409\n",
      "Epoch 9/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6162 - acc: 0.7647 - val_loss: 0.4242 - val_acc: 0.8352\n",
      "Epoch 10/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6304 - acc: 0.7728 - val_loss: 0.3068 - val_acc: 0.8409\n",
      "Epoch 11/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5955 - acc: 0.7906 - val_loss: 0.3150 - val_acc: 0.8523\n",
      "Epoch 12/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.6197 - acc: 0.7706 - val_loss: 0.3109 - val_acc: 0.8580\n",
      "Epoch 13/50\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.6111 - acc: 0.7752 - val_loss: 0.3017 - val_acc: 0.8466\n",
      "Epoch 14/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6208 - acc: 0.7735 - val_loss: 0.2934 - val_acc: 0.8807\n",
      "Epoch 15/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5938 - acc: 0.7894 - val_loss: 0.3589 - val_acc: 0.8295\n",
      "Epoch 16/50\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5926 - acc: 0.7877 - val_loss: 0.3400 - val_acc: 0.8523\n",
      "Epoch 17/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.6210 - acc: 0.7854 - val_loss: 0.3246 - val_acc: 0.8466\n",
      "Epoch 18/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5679 - acc: 0.7874 - val_loss: 0.3155 - val_acc: 0.8523\n",
      "Epoch 19/50\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.5895 - acc: 0.7824 - val_loss: 0.3332 - val_acc: 0.8466\n",
      "Epoch 20/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5634 - acc: 0.7936 - val_loss: 0.3071 - val_acc: 0.8295\n",
      "Epoch 21/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5737 - acc: 0.7866 - val_loss: 0.3103 - val_acc: 0.8580\n",
      "Epoch 22/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5746 - acc: 0.8029 - val_loss: 0.3096 - val_acc: 0.8807\n",
      "Epoch 23/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5655 - acc: 0.7982 - val_loss: 0.3036 - val_acc: 0.8466\n",
      "Epoch 24/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5630 - acc: 0.7971 - val_loss: 0.3751 - val_acc: 0.8295\n",
      "Epoch 25/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5507 - acc: 0.7999 - val_loss: 0.2888 - val_acc: 0.8693\n",
      "Epoch 26/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5811 - acc: 0.7906 - val_loss: 0.2878 - val_acc: 0.8864\n",
      "Epoch 27/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5639 - acc: 0.8000 - val_loss: 0.3456 - val_acc: 0.8409\n",
      "Epoch 28/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5519 - acc: 0.8035 - val_loss: 0.3013 - val_acc: 0.8352\n",
      "Epoch 29/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5408 - acc: 0.7994 - val_loss: 0.2824 - val_acc: 0.8693\n",
      "Epoch 30/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5676 - acc: 0.7971 - val_loss: 0.3257 - val_acc: 0.8807\n",
      "Epoch 31/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5245 - acc: 0.8095 - val_loss: 0.3326 - val_acc: 0.8580\n",
      "Epoch 32/50\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5623 - acc: 0.8037 - val_loss: 0.4076 - val_acc: 0.8011\n",
      "Epoch 33/50\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5425 - acc: 0.8012 - val_loss: 0.3735 - val_acc: 0.8466\n",
      "Epoch 34/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5201 - acc: 0.8206 - val_loss: 0.3486 - val_acc: 0.8352\n",
      "Epoch 35/50\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5477 - acc: 0.8101 - val_loss: 0.3869 - val_acc: 0.8466\n",
      "Epoch 36/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5455 - acc: 0.8077 - val_loss: 0.4879 - val_acc: 0.8125\n",
      "Epoch 37/50\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.5336 - acc: 0.8048 - val_loss: 0.3392 - val_acc: 0.8409\n",
      "Epoch 38/50\n",
      "106/106 [==============================] - 2s 16ms/step - loss: 0.5285 - acc: 0.8177 - val_loss: 0.4152 - val_acc: 0.8523\n",
      "Epoch 39/50\n",
      "106/106 [==============================] - 2s 15ms/step - loss: 0.5205 - acc: 0.8071 - val_loss: 0.3883 - val_acc: 0.8352\n",
      "Epoch 40/50\n",
      "106/106 [==============================] - 2s 19ms/step - loss: 0.5053 - acc: 0.8189 - val_loss: 0.4179 - val_acc: 0.8182\n",
      "Epoch 41/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5461 - acc: 0.8143 - val_loss: 0.5202 - val_acc: 0.8125\n",
      "Epoch 42/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5192 - acc: 0.8076 - val_loss: 0.5508 - val_acc: 0.8239\n",
      "Epoch 43/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5285 - acc: 0.8263 - val_loss: 0.5450 - val_acc: 0.8295\n",
      "Epoch 44/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5223 - acc: 0.8154 - val_loss: 0.4574 - val_acc: 0.8295\n",
      "Epoch 45/50\n",
      "106/106 [==============================] - 2s 17ms/step - loss: 0.5097 - acc: 0.8099 - val_loss: 0.5114 - val_acc: 0.8523\n",
      "Epoch 46/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.4975 - acc: 0.8213 - val_loss: 0.5482 - val_acc: 0.8068\n",
      "Epoch 47/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5215 - acc: 0.8099 - val_loss: 0.4394 - val_acc: 0.8409\n",
      "Epoch 48/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5381 - acc: 0.8200 - val_loss: 0.4682 - val_acc: 0.8295\n",
      "Epoch 49/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5132 - acc: 0.8182 - val_loss: 0.4835 - val_acc: 0.8295\n",
      "Epoch 50/50\n",
      "106/106 [==============================] - 2s 18ms/step - loss: 0.5084 - acc: 0.8206 - val_loss: 0.5597 - val_acc: 0.8295\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 25, 25\n",
    "\n",
    "train_data_dir = 'CNN/train'\n",
    "validation_data_dir = 'CNN/test'\n",
    "test_data_dir = 'CNN/actual_testing'\n",
    "\n",
    "nb_train_samples = 1711\n",
    "nb_validation_samples = 178\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)\n",
    "\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'laying_down': 0, 'sitting': 1, 'standing': 2, 'walking': 3}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = (train_generator.class_indices)\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 260 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_dir = 'CNN/actual_testing'\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    #batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "predict = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = [[1,0,0,0]]*len(color_map_spiral['laying_down'])+[[0,1,0,0]]*len(color_map_spiral['sitting'])+[[0,0,1,0]]*len(color_map_spiral['standing'])+[[0,0,0,1]]*len(color_map_spiral['walking'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Precision: ', array([0.48051948, 0.37209302, 0.14705882, 0.125     ]))\n",
      "0.27692307692307694\n"
     ]
    }
   ],
   "source": [
    "#print predict\n",
    "\n",
    "def get_hot_value(my_list):\n",
    "        max_val = max(my_list)\n",
    "        return [int(item == max_val) for item in my_list]\n",
    "\n",
    "hot_list = [get_hot_value(sublist) for sublist in predict]\n",
    "\n",
    "t  = np.array(y_test)\n",
    "p  = np.array(hot_list)\n",
    "t = np.argmax(t, axis=1)\n",
    "p = np.argmax(p, axis=1)\n",
    "\n",
    "#print hot_list\n",
    "print(\"Precision: \", precision_score(t, p, average=None))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "260\n"
     ]
    }
   ],
   "source": [
    "print len(predict)\n",
    "print len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0990019e-07, 4.2163174e-05, 1.4948982e-02, 8.5364991e-01],\n",
       "       [3.1117118e-09, 4.0054915e-06, 2.2853166e-03, 9.4797361e-01],\n",
       "       [2.6103375e-09, 4.0909013e-06, 3.2966968e-03, 9.4010174e-01],\n",
       "       ...,\n",
       "       [5.2023053e-01, 1.7847300e-02, 9.1647869e-04, 8.2183027e-07],\n",
       "       [6.1969858e-01, 9.8320469e-03, 5.1267534e-02, 7.5963067e-06],\n",
       "       [4.1519919e-01, 2.3386780e-02, 2.6436889e-01, 9.7757904e-05]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hot_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
