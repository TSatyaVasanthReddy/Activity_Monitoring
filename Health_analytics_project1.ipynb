{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features for accelerometer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "Found pickle files for accelerometer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks_cwt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "in_dir = \"data_3_4_2018\"\n",
    "\n",
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "def number_of_peaks(window):\n",
    "    indexes = find_peaks_cwt(window, np.arange(1, len(window)))\n",
    "\n",
    "    return len(indexes)\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "pickle_file = Path(\"pickles/accelerometer_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for accelerometer\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/accelerometer_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/accelerometer_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/accelerometer_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "            print(\"/\".join(path) + \"/\" + f)\n",
    "\n",
    "            full_path = \"/\".join(path) + \"/\" + f\n",
    "\n",
    "            if \"gyroscope\" in full_path:\n",
    "                print(\"Skip \", full_path)\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(full_path, header=None)\n",
    "\n",
    "            print(\"Before trimming: \", df.shape)\n",
    "            \n",
    "            df = trim_first_last_n_seconds(df, trim_num_seconds, acc_freq)\n",
    "            if df is None:\n",
    "                print(\"Continuing\")\n",
    "                continue\n",
    "\n",
    "            print(\"After trimming: \", df.shape)\n",
    "\n",
    "            #Sample the data according to the size of the window with 50% overlap\n",
    "            for index in range(0, df.shape[0]-window_size, window_jump_steps):\n",
    "                indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                window = df.iloc[indexes,:]\n",
    "\n",
    "                #Generate the features for this window\n",
    "                mean_x = np.mean(window[1].tolist())\n",
    "                mean_y = np.mean(window[2].tolist())\n",
    "                mean_z = np.mean(window[3].tolist())\n",
    "\n",
    "                var_x = np.var(window[1].tolist())\n",
    "                var_y = np.var(window[2].tolist())\n",
    "                var_z = np.var(window[3].tolist())\n",
    "\n",
    "                num_peaks_x = number_of_peaks(window[1].tolist())\n",
    "                num_peaks_y = number_of_peaks(window[2].tolist())\n",
    "                num_peaks_z = number_of_peaks(window[3].tolist())            \n",
    "\n",
    "                #Store the features\n",
    "                window_feature = []\n",
    "                window_feature.append(mean_x)\n",
    "                window_feature.append(mean_y)\n",
    "                window_feature.append(mean_z)\n",
    "                window_feature.append(var_x)\n",
    "                window_feature.append(var_y)\n",
    "                window_feature.append(var_z)\n",
    "                window_feature.append(num_peaks_x)\n",
    "                window_feature.append(num_peaks_y)\n",
    "                window_feature.append(num_peaks_z)\n",
    "\n",
    "                features.append(window_feature)\n",
    "\n",
    "                #Store the label\n",
    "                labels.append(df[5].iloc[1])\n",
    "\n",
    "            dfs_list.append(df)\n",
    "\n",
    "    dfs = pd.concat(dfs_list)\n",
    "\n",
    "    pickle.dump(features, open(\"pickles/accelerometer_features.pickle\", \"wb\"))\n",
    "    pickle.dump(labels, open(\"pickles/accelerometer_labels.pickle\", \"wb\"))\n",
    "    pickle.dump(dfs_list, open(\"pickles/accelerometer_dfs_list.pickle\", \"wb\"))\n",
    "\n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute LSTM features for accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "data_3_4_2018/0_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64964, 6)\n",
      "After trimming:  (59965, 6)\n",
      "data_3_4_2018/10_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64675, 6)\n",
      "After trimming:  (59676, 6)\n",
      "data_3_4_2018/11_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (66938, 6)\n",
      "After trimming:  (61939, 6)\n",
      "data_3_4_2018/12_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (60980, 6)\n",
      "After trimming:  (55981, 6)\n",
      "data_3_4_2018/13_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (62506, 6)\n",
      "After trimming:  (57507, 6)\n",
      "data_3_4_2018/14_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65108, 6)\n",
      "After trimming:  (60109, 6)\n",
      "data_3_4_2018/15_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (62588, 6)\n",
      "After trimming:  (57589, 6)\n",
      "data_3_4_2018/16_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (82181, 6)\n",
      "After trimming:  (77182, 6)\n",
      "data_3_4_2018/17_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (83784, 6)\n",
      "After trimming:  (78785, 6)\n",
      "data_3_4_2018/18_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (85978, 6)\n",
      "After trimming:  (80979, 6)\n",
      "data_3_4_2018/19_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (101168, 6)\n",
      "After trimming:  (96169, 6)\n",
      "data_3_4_2018/1_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (50274, 6)\n",
      "After trimming:  (45275, 6)\n",
      "data_3_4_2018/20_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (53812, 6)\n",
      "After trimming:  (48813, 6)\n",
      "data_3_4_2018/21_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65720, 6)\n",
      "After trimming:  (60721, 6)\n",
      "data_3_4_2018/22_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (71074, 6)\n",
      "After trimming:  (66075, 6)\n",
      "data_3_4_2018/23_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (30053, 6)\n",
      "After trimming:  (25054, 6)\n",
      "data_3_4_2018/24_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (26070, 6)\n",
      "After trimming:  (21071, 6)\n",
      "data_3_4_2018/25_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (718108, 6)\n",
      "After trimming:  (713109, 6)\n",
      "data_3_4_2018/26_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (66885, 6)\n",
      "After trimming:  (61886, 6)\n",
      "data_3_4_2018/27_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (68901, 6)\n",
      "After trimming:  (63902, 6)\n",
      "data_3_4_2018/28_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (69512, 6)\n",
      "After trimming:  (64513, 6)\n",
      "data_3_4_2018/29_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (67293, 6)\n",
      "After trimming:  (62294, 6)\n",
      "data_3_4_2018/2_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (59060, 6)\n",
      "After trimming:  (54061, 6)\n",
      "data_3_4_2018/30_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (67641, 6)\n",
      "After trimming:  (62642, 6)\n",
      "data_3_4_2018/31_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (40200, 6)\n",
      "After trimming:  (35201, 6)\n",
      "data_3_4_2018/32_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (118805, 6)\n",
      "After trimming:  (113806, 6)\n",
      "data_3_4_2018/33_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65634, 6)\n",
      "After trimming:  (60635, 6)\n",
      "data_3_4_2018/34_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (98968, 6)\n",
      "After trimming:  (93969, 6)\n",
      "data_3_4_2018/35_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (96365, 6)\n",
      "After trimming:  (91366, 6)\n",
      "data_3_4_2018/36_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (2323, 6)\n",
      "Continuing\n",
      "data_3_4_2018/37_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (43060, 6)\n",
      "After trimming:  (38061, 6)\n",
      "data_3_4_2018/38_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (83999, 6)\n",
      "After trimming:  (79000, 6)\n",
      "data_3_4_2018/39_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65899, 6)\n",
      "After trimming:  (60900, 6)\n",
      "data_3_4_2018/3_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (55316, 6)\n",
      "After trimming:  (50317, 6)\n",
      "data_3_4_2018/40_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (89512, 6)\n",
      "After trimming:  (84513, 6)\n",
      "data_3_4_2018/41_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (59739, 6)\n",
      "After trimming:  (54740, 6)\n",
      "data_3_4_2018/42_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (57828, 6)\n",
      "After trimming:  (52829, 6)\n",
      "data_3_4_2018/43_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (35267, 6)\n",
      "After trimming:  (30268, 6)\n",
      "data_3_4_2018/44_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (52974, 6)\n",
      "After trimming:  (47975, 6)\n",
      "data_3_4_2018/4_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (48531, 6)\n",
      "After trimming:  (43532, 6)\n",
      "data_3_4_2018/5_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (63094, 6)\n",
      "After trimming:  (58095, 6)\n",
      "data_3_4_2018/6_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (63971, 6)\n",
      "After trimming:  (58972, 6)\n",
      "data_3_4_2018/7_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64212, 6)\n",
      "After trimming:  (59213, 6)\n",
      "data_3_4_2018/8_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64175, 6)\n",
      "After trimming:  (59176, 6)\n",
      "data_3_4_2018/9_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (52095, 6)\n",
      "After trimming:  (47096, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_dir = \"data_3_4_2018\"\n",
    "\n",
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "pickle_file = Path(\"pickles/lstm_acc_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for LSTM acc and gyro\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/lstm_acc_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/lstm_acc_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/lstm_acc_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            if 'accelerometer' in f:\n",
    "                accelerometer_path = \"/\".join(path) + \"/\" + f\n",
    "                print(accelerometer_path)\n",
    "\n",
    "                df = pd.read_csv(accelerometer_path, header=None)\n",
    "\n",
    "                print(\"Before trimming: \", df.shape)\n",
    "\n",
    "                df = trim_first_last_n_seconds(df, trim_num_seconds, acc_freq)\n",
    "                if df is None:\n",
    "                    print(\"Continuing\")\n",
    "                    continue\n",
    "\n",
    "                print(\"After trimming: \", df.shape)\n",
    "\n",
    "                #Sample the data according to the size of the window with 50% overlap\n",
    "                for index in range(0, df.shape[0]-window_size, window_jump_steps):\n",
    "                    indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                    window = df.iloc[indexes, 1:df.shape[1]-2]\n",
    "\n",
    "                    #Generate the features for this window            \n",
    "                    features.append(np.asarray(window))\n",
    "\n",
    "                    #Store the label\n",
    "                    labels.append(df[5].iloc[0])\n",
    "\n",
    "                dfs_list.append(df)\n",
    "\n",
    "    dfs = pd.concat(dfs_list)\n",
    "    \n",
    "    pickle.dump(features, open(\"pickles/lstm_acc_features.pickle\", \"wb\"))\n",
    "    pickle.dump(labels, open(\"pickles/lstm_acc_labels.pickle\", \"wb\"))\n",
    "    pickle.dump(dfs_list, open(\"pickles/lstm_acc_dfs_list.pickle\", \"wb\"))\n",
    "    \n",
    "    \n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute LSTM features for accelerometer and gyroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "Found pickle files for LSTM acc and gyro\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "in_dir = \"data_3_4_2018\"\n",
    "\n",
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def combine_acc_gyro_data(df_acc, df_gyro):\n",
    "    threshold = 10\n",
    "    \n",
    "    acc_index = 0\n",
    "    gyro_index = 0\n",
    "    \n",
    "    acc_matrix = df_acc.as_matrix()\n",
    "    gyro_matrix = df_gyro.as_matrix()\n",
    "    \n",
    "    combined_list = []\n",
    "    \n",
    "    while (acc_index < df_acc.shape[0]) and (gyro_index < df_gyro.shape[0]):\n",
    "        #find next gyro_index within threshold and append the data\n",
    "        \n",
    "        acc_time = df_acc.iloc[acc_index][0]\n",
    "        gyro_time = df_gyro.iloc[gyro_index][0]\n",
    "\n",
    "        if (acc_time < gyro_time):\n",
    "            while (acc_index < df_acc.shape[0]) and (gyro_time - acc_time > threshold):\n",
    "                acc_time = acc_matrix[acc_index][0] #df_acc.iloc[acc_index][0]\n",
    "                acc_index += 1\n",
    "        else:\n",
    "            while (gyro_index < df_gyro.shape[0]) and (acc_time - gyro_time > threshold):\n",
    "                gyro_time = gyro_matrix[gyro_index][0] #df_gyro.iloc[gyro_index][0]\n",
    "                gyro_index += 1\n",
    "            \n",
    "        combined_list.append(np.concatenate((acc_matrix[acc_index][:4], gyro_matrix[gyro_index][1:6]), axis=0))\n",
    "        acc_index += 1\n",
    "        gyro_index += 1\n",
    "            \n",
    "    return combined_list\n",
    "\n",
    "\n",
    "pickle_file = Path(\"pickles/lstm_acc_gyro_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for LSTM acc and gyro\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/lstm_acc_gyro_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/lstm_acc_gyro_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/lstm_acc_gyro_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            if 'accelerometer' in f:\n",
    "                accelerometer_path = \"/\".join(path) + \"/\" + f\n",
    "                print(accelerometer_path)\n",
    "\n",
    "                first_ = f.find(\"_\")\n",
    "                g = f[0:first_] + \"_4\" + f[first_+2:]\n",
    "                gyroscope_path = \"/\".join(path) + \"/\" + g.replace(\"accelerometer\",\"gyroscope\")\n",
    "                print(gyroscope_path)\n",
    "\n",
    "                df_acc = pd.read_csv(accelerometer_path, header=None)\n",
    "                df_gyro = pd.read_csv(gyroscope_path, header=None)\n",
    "\n",
    "                combined_list = combine_acc_gyro_data(df_acc, df_gyro)\n",
    "                combined_numpy = np.array(combined_list)\n",
    "\n",
    "                combined_df = pd.DataFrame(data=combined_numpy)\n",
    "\n",
    "                print(\"Before trimming: \", combined_df.shape)\n",
    "\n",
    "                combined_df = trim_first_last_n_seconds(combined_df, trim_num_seconds, acc_freq)\n",
    "                if combined_df is None:\n",
    "                    print(\"Continuing\")\n",
    "                    continue\n",
    "\n",
    "                print(\"After trimming: \", combined_df.shape)\n",
    "\n",
    "                #Sample the data according to the size of the window with 50% overlap\n",
    "                for index in range(0, combined_df.shape[0]-window_size, window_jump_steps):\n",
    "                    indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                    window = combined_df.iloc[indexes, 1:7] #,1:combined_df.shape[1]-2]\n",
    "\n",
    "                    #Generate the features for this window            \n",
    "                    features.append(np.asarray(window))\n",
    "\n",
    "                    #Store the label\n",
    "                    labels.append(combined_df[8].iloc[0])\n",
    "\n",
    "                dfs_list.append(combined_df)\n",
    "\n",
    "    dfs = pd.concat(dfs_list)\n",
    "    \n",
    "    pickle.dump(features, open(\"pickles/lstm_acc_gyro_features.pickle\", \"wb\"))\n",
    "    pickle.dump(labels, open(\"pickles/lstm_acc_gyro_labels.pickle\", \"wb\"))\n",
    "    pickle.dump(dfs_list, open(\"pickles/lstm_acc_gyro_dfs_list.pickle\", \"wb\"))\n",
    "    \n",
    "    \n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def getModel(modelName, args):\n",
    "    \n",
    "    if modelName == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        \n",
    "    if modelName == 'SVC':\n",
    "        model = SVC(random_state=42, kernel=args[0], C=args[1], decision_function_shape=args[2])\n",
    "        \n",
    "    if modelName == 'DecisionTreeClassifier':\n",
    "        model = DecisionTreeClassifier(random_state=42, max_features=args[0], criterion=args[1])\n",
    "        \n",
    "    if modelName == 'RandomForestClassifier':\n",
    "        model = RandomForestClassifier(n_estimators=args[0], criterion=args[1], max_features=args[2], max_depth=args[3], oob_score=True, random_state=42)\n",
    "        \n",
    "    if modelName == 'MLPClassifier':\n",
    "        model = MLPClassifier(hidden_layer_sizes=args[0], activation=args[1], solver=args[2], random_state=42, max_iter=500)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "\n",
    "def run_kfold(features, labels, num_splits, modelName, args=None, verbose=False):\n",
    "    \n",
    "    X = np.array(normalize(features))\n",
    "    y = np.array(labels)\n",
    "\n",
    "    kf = KFold(n_splits=num_splits, random_state=None, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    foldAccuracy = list()\n",
    "    foldPrecision = list()\n",
    "    foldRecall = list()\n",
    "    bestModel = None\n",
    "    bestAccuracy = float(sys.maxsize) * (-1)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = getModel(modelName, args)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        prediction = model.predict(X_test)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nFold: \", fold)\n",
    "            print(\"Confusion Matrix:\")\n",
    "            cm = metrics.confusion_matrix(yTest, prediction)\n",
    "            print(cm)\n",
    "            plt.matshow(cm, cmap = plt.cm.Oranges)\n",
    "            plt.title('Confusion matrix')\n",
    "            plt.colorbar()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.show()\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_test, prediction)\n",
    "        precision = metrics.precision_score(y_test, prediction, average = None)\n",
    "        recall = metrics.recall_score(y_test, prediction, average = None)\n",
    "\n",
    "        foldAccuracy.append(accuracy)\n",
    "        foldPrecision.append(precision)\n",
    "        foldRecall.append(recall)\n",
    "        \n",
    "        if accuracy > bestAccuracy:\n",
    "            bestAccuracy = accuracy\n",
    "            bestModel = model\n",
    "                \n",
    "    print(\"\\nBest Accuracy: \", bestAccuracy)\n",
    "    \n",
    "    return bestModel, foldAccuracy, foldPrecision, foldRecall        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.7570449352627571\n"
     ]
    }
   ],
   "source": [
    "num_splits = 5\n",
    "\n",
    "lrModel, lrAccuracy, lrPrecision, lrRecall = run_kfold(features, labels, num_splits, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8316831683168316\n"
     ]
    }
   ],
   "source": [
    "#hard margin SVM 1\n",
    "\n",
    "#kernel = 'rbf'\n",
    "#C = 1000\n",
    "#decision_function_shape = 'ovo'\n",
    "\n",
    "num_splits = 5\n",
    "args = ['rbf', 1000, 'ovo']\n",
    "\n",
    "hmSVC1Model, hmSVC1Accuracy, hmSVC1Precision, hmSVC1Recall = run_kfold(features, labels, num_splits, 'SVC', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8651942117288652\n"
     ]
    }
   ],
   "source": [
    "#decision Tree 1\n",
    "\n",
    "#max_features = 6\n",
    "#criterion = 'gini'\n",
    "\n",
    "num_splits = 5\n",
    "args = [6, 'gini']\n",
    "\n",
    "dt1Model, dt1Accuracy, dt1Precision, dt1Recall = run_kfold(features, labels, num_splits, 'DecisionTreeClassifier', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8316831683168316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "#random Forest Classification 1\n",
    "\n",
    "#n_estimators = 20\n",
    "#criterion = gini\n",
    "#max_features = 6\n",
    "#max_depth = 5\n",
    "\n",
    "num_splits = 5\n",
    "args = [20, 'gini', 6, 5]\n",
    "\n",
    "rf1Model, rf1Accuracy, rf1Precision, rf1Recall = run_kfold(features, labels, num_splits, 'RandomForestClassifier', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8888042650418888\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron Classification 2\n",
    "\n",
    "#hidden_layer_sizes = 50\n",
    "#activation = tanh\n",
    "#solver = lbfgs\n",
    "\n",
    "num_splits = 5\n",
    "args = [50, 'tanh', 'lbfgs']\n",
    "\n",
    "mlp2Model, mlp2Accuracy, mlp2Precision, mlp2Recall = run_kfold(features, labels, num_splits, 'MLPClassifier', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, LSTMCell\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def run_lstm(features, labels, train_test_split_ratio, batch_size, epochs):\n",
    "\n",
    "    X = features\n",
    "\n",
    "    y = np.array(labels)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    y = y.reshape(-1,1)\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    y = ohe.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_test_split_ratio, random_state=42)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(X_train[0].shape[0], X_train[0].shape[1]), return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    #Evaluate the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    predicted_labels = model.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy: \", scores[1]*100)\n",
    "\n",
    "    def get_hot_value(my_list):\n",
    "        max_val = max(my_list)\n",
    "        return [int(item == max_val) for item in my_list]\n",
    "\n",
    "    hot_list = [get_hot_value(sublist) for sublist in predicted_labels]\n",
    "\n",
    "    t  = np.array(y_test)\n",
    "    p  = np.array(hot_list)\n",
    "    t = np.argmax(t, axis=1)\n",
    "    p = np.argmax(p, axis=1)\n",
    "\n",
    "    print(\"Precision: \", precision_score(t, p, average=None))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1000, 100)         42800     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 123,604\n",
      "Trainable params: 123,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      " 600/3808 [===>..........................] - ETA: 1:39 - loss: 1.4339 - acc: 0.2633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-19b097efe5e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrun_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-4e81fe15c768>\u001b[0m in \u001b[0;36mrun_lstm\u001b[1;34m(features, labels, train_test_split_ratio, batch_size, epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m#Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test_split_ratio = 0.1\n",
    "batch_size = 300\n",
    "epochs = 100\n",
    "\n",
    "run_lstm(features, labels, train_test_split_ratio, batch_size, epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
