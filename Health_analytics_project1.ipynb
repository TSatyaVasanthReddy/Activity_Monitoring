{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks_cwt\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.stats import iqr\n",
    "from pyentrp import entropy as ent\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, LSTMCell\n",
    "from keras.layers import Dropout\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = \"data_3_7_2018\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute features for accelerometer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "data_3_7_2018/0_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64964, 6)\n",
      "After trimming:  (59965, 6)\n",
      "data_3_7_2018/0_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/0_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/10_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64675, 6)\n",
      "After trimming:  (59676, 6)\n",
      "data_3_7_2018/10_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/10_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/11_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (66938, 6)\n",
      "After trimming:  (61939, 6)\n",
      "data_3_7_2018/11_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/11_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/12_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (60980, 6)\n",
      "After trimming:  (55981, 6)\n",
      "data_3_7_2018/12_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/12_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/13_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (62506, 6)\n",
      "After trimming:  (57507, 6)\n",
      "data_3_7_2018/13_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/13_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/14_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65108, 6)\n",
      "After trimming:  (60109, 6)\n",
      "data_3_7_2018/14_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/14_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/15_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (62588, 6)\n",
      "After trimming:  (57589, 6)\n",
      "data_3_7_2018/15_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/15_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/16_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (82181, 6)\n",
      "After trimming:  (77182, 6)\n",
      "data_3_7_2018/16_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/16_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/17_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (83784, 6)\n",
      "After trimming:  (78785, 6)\n",
      "data_3_7_2018/17_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/17_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/18_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (85978, 6)\n",
      "After trimming:  (80979, 6)\n",
      "data_3_7_2018/18_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/18_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/19_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (101168, 6)\n",
      "After trimming:  (96169, 6)\n",
      "data_3_7_2018/19_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/19_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/1_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (50274, 6)\n",
      "After trimming:  (45275, 6)\n",
      "data_3_7_2018/1_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/1_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/20_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (53812, 6)\n",
      "After trimming:  (48813, 6)\n",
      "data_3_7_2018/20_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/20_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/21_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65720, 6)\n",
      "After trimming:  (60721, 6)\n",
      "data_3_7_2018/21_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/21_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/22_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (71074, 6)\n",
      "After trimming:  (66075, 6)\n",
      "data_3_7_2018/22_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/22_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/23_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (30053, 6)\n",
      "After trimming:  (25054, 6)\n",
      "data_3_7_2018/23_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/23_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/24_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (26070, 6)\n",
      "After trimming:  (21071, 6)\n",
      "data_3_7_2018/24_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/24_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/25_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (66885, 6)\n",
      "After trimming:  (61886, 6)\n",
      "data_3_7_2018/25_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/25_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/26_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (68901, 6)\n",
      "After trimming:  (63902, 6)\n",
      "data_3_7_2018/26_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/26_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/27_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (69512, 6)\n",
      "After trimming:  (64513, 6)\n",
      "data_3_7_2018/27_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/27_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/28_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (67293, 6)\n",
      "After trimming:  (62294, 6)\n",
      "data_3_7_2018/28_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/28_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/29_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (67641, 6)\n",
      "After trimming:  (62642, 6)\n",
      "data_3_7_2018/29_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/29_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/2_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (59060, 6)\n",
      "After trimming:  (54061, 6)\n",
      "data_3_7_2018/2_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/2_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/30_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (40200, 6)\n",
      "After trimming:  (35201, 6)\n",
      "data_3_7_2018/30_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/30_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/31_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (118805, 6)\n",
      "After trimming:  (113806, 6)\n",
      "data_3_7_2018/31_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/31_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/32_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65634, 6)\n",
      "After trimming:  (60635, 6)\n",
      "data_3_7_2018/32_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/32_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/33_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (98968, 6)\n",
      "After trimming:  (93969, 6)\n",
      "data_3_7_2018/33_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/33_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/34_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (96365, 6)\n",
      "After trimming:  (91366, 6)\n",
      "data_3_7_2018/34_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/34_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/35_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (2323, 6)\n",
      "Continuing\n",
      "data_3_7_2018/35_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/35_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/36_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (43060, 6)\n",
      "After trimming:  (38061, 6)\n",
      "data_3_7_2018/36_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/36_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/37_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (83999, 6)\n",
      "After trimming:  (79000, 6)\n",
      "data_3_7_2018/37_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/37_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/38_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65899, 6)\n",
      "After trimming:  (60900, 6)\n",
      "data_3_7_2018/38_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/38_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/39_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (89512, 6)\n",
      "After trimming:  (84513, 6)\n",
      "data_3_7_2018/39_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/39_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/3_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (55316, 6)\n",
      "After trimming:  (50317, 6)\n",
      "data_3_7_2018/3_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/3_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/40_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (59739, 6)\n",
      "After trimming:  (54740, 6)\n",
      "data_3_7_2018/40_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/40_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/41_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (57828, 6)\n",
      "After trimming:  (52829, 6)\n",
      "data_3_7_2018/41_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/41_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/42_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (35267, 6)\n",
      "After trimming:  (30268, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_3_7_2018/42_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/42_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/43_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (52974, 6)\n",
      "After trimming:  (47975, 6)\n",
      "data_3_7_2018/43_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/43_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/44_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (147020, 6)\n",
      "After trimming:  (142021, 6)\n",
      "data_3_7_2018/44_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/44_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/45_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (99784, 6)\n",
      "After trimming:  (94785, 6)\n",
      "data_3_7_2018/45_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/45_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/46_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (9650, 6)\n",
      "After trimming:  (4651, 6)\n",
      "data_3_7_2018/46_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/46_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/47_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (73355, 6)\n",
      "After trimming:  (68356, 6)\n",
      "data_3_7_2018/47_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/47_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/48_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (177495, 6)\n",
      "After trimming:  (172496, 6)\n",
      "data_3_7_2018/48_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/48_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/49_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (165062, 6)\n",
      "After trimming:  (160063, 6)\n",
      "data_3_7_2018/49_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/49_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/4_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (48531, 6)\n",
      "After trimming:  (43532, 6)\n",
      "data_3_7_2018/4_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/4_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/50_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (355411, 6)\n",
      "After trimming:  (350412, 6)\n",
      "data_3_7_2018/50_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/50_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/51_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (149453, 6)\n",
      "After trimming:  (144454, 6)\n",
      "data_3_7_2018/51_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/51_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/52_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (138601, 6)\n",
      "After trimming:  (133602, 6)\n",
      "data_3_7_2018/52_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/52_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/53_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (106594, 6)\n",
      "After trimming:  (101595, 6)\n",
      "data_3_7_2018/53_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/53_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/54_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (168816, 6)\n",
      "After trimming:  (163817, 6)\n",
      "data_3_7_2018/54_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/54_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/5_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (63094, 6)\n",
      "After trimming:  (58095, 6)\n",
      "data_3_7_2018/5_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/5_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/6_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (63971, 6)\n",
      "After trimming:  (58972, 6)\n",
      "data_3_7_2018/6_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/6_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/7_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64212, 6)\n",
      "After trimming:  (59213, 6)\n",
      "data_3_7_2018/7_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/7_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/8_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64175, 6)\n",
      "After trimming:  (59176, 6)\n",
      "data_3_7_2018/8_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/8_4_android.sensor.gyroscope.data.csv\n",
      "data_3_7_2018/9_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (52095, 6)\n",
      "After trimming:  (47096, 6)\n",
      "data_3_7_2018/9_4_android.sensor.gyroscope.data.csv\n",
      "Skip  data_3_7_2018/9_4_android.sensor.gyroscope.data.csv\n"
     ]
    }
   ],
   "source": [
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "#Helper Functions\n",
    "\n",
    "def Signal_magnitude_area(x,y,z):\n",
    "    \n",
    "    sum = 0    \n",
    "    for i in range(len(x)):\n",
    "        sum += (abs(x[i]) + abs(y[i]) + abs(z[i]))\n",
    "        \n",
    "    return float(sum)/len(x)\n",
    "\n",
    "\n",
    "def Power(x):\n",
    "    \n",
    "    power = (LA.norm(x)**2)/ len(x)\n",
    "    return power\n",
    "    \n",
    "def number_of_peaks(window):\n",
    "    indexes = find_peaks_cwt(window, np.arange(1, len(window)))\n",
    "\n",
    "    return len(indexes)\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "pickle_file = Path(\"pickles/accelerometer_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for accelerometer\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/accelerometer_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/accelerometer_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/accelerometer_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "            print(\"/\".join(path) + \"/\" + f)\n",
    "\n",
    "            full_path = \"/\".join(path) + \"/\" + f\n",
    "\n",
    "            if \"gyroscope\" in full_path:\n",
    "                print(\"Skip \", full_path)\n",
    "                continue\n",
    "\n",
    "            df = pd.read_csv(full_path, header=None)\n",
    "\n",
    "            print(\"Before trimming: \", df.shape)\n",
    "            \n",
    "            df = trim_first_last_n_seconds(df, trim_num_seconds, acc_freq)\n",
    "            if df is None:\n",
    "                print(\"Continuing\")\n",
    "                continue\n",
    "\n",
    "            print(\"After trimming: \", df.shape)\n",
    "\n",
    "            #Sample the data according to the size of the window with 50% overlap\n",
    "            for index in range(0, df.shape[0]-window_size, window_jump_steps):\n",
    "                indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                window = df.iloc[indexes,:]\n",
    "\n",
    "                X_list = window[1].tolist()\n",
    "                Y_list = window[2].tolist()\n",
    "                Z_list = window[3].tolist()\n",
    "                \n",
    "                \n",
    "                #Generate the features for this window\n",
    "                \n",
    "                \n",
    "       # ****************** Time-Domain Features ************************* #\n",
    "    \n",
    "                #Mean of the signals\n",
    "                mean_x = np.mean(X_list)\n",
    "                mean_y = np.mean(Y_list)\n",
    "                mean_z = np.mean(Z_list)\n",
    "\n",
    "                #Variance of the signals\n",
    "                var_x = np.var(X_list)\n",
    "                var_y = np.var(Y_list)\n",
    "                var_z = np.var(Z_list)\n",
    "\n",
    "                #Number of peaks in the signals\n",
    "                #num_peaks_x = number_of_peaks(X_list)\n",
    "                #num_peaks_y = number_of_peaks(Y_list)\n",
    "                #num_peaks_z = number_of_peaks(Z_list)            \n",
    "\n",
    "                #Median of the signals\n",
    "                median_x = np.ma.median(X_list)\n",
    "                median_y = np.ma.median(Y_list)\n",
    "                median_z = np.ma.median(Z_list)\n",
    "              \n",
    "                #Standard Deviation of the signals\n",
    "                std_x = np.std(X_list)\n",
    "                std_y = np.std(Y_list)\n",
    "                std_z = np.std(Z_list)\n",
    "                               \n",
    "                #Compute Signal Magnitude Area\n",
    "                signal_mag_area = Signal_magnitude_area(X_list, Y_list, Z_list)\n",
    "                               \n",
    "                #Maximum and Minimum values and their indexes\n",
    "                max_x = max(X_list)\n",
    "                max_index_x = X_list.index(max_x)               \n",
    "                min_x = min(X_list)\n",
    "                min_index_x = X_list.index(min_x)\n",
    "               \n",
    "                max_y = max(Y_list)\n",
    "                max_index_y = Y_list.index(max_y)              \n",
    "                min_y = min(Y_list)\n",
    "                min_index_y = Y_list.index(min_y)               \n",
    "                \n",
    "                max_z = max(Z_list)\n",
    "                max_index_z = Z_list.index(max_z)             \n",
    "                min_z = min(Z_list)\n",
    "                min_index_z = Z_list.index(min_z)\n",
    "                \n",
    "                \n",
    "                #Power of X,Y and Z signals             \n",
    "                power_x = Power(X_list)\n",
    "                power_y = Power(Y_list)\n",
    "                power_z = Power(Z_list)\n",
    "                \n",
    "                \n",
    "                #Skewness and Kurtosis\n",
    "                skew_x = skew(X_list)\n",
    "                skew_y = skew(Y_list)\n",
    "                skew_z = skew(Z_list)\n",
    "                \n",
    "                kurtosis_x = kurtosis(X_list)                \n",
    "                kurtosis_y = kurtosis(Y_list)\n",
    "                kurtosis_z = kurtosis(Z_list)\n",
    "                \n",
    "                \n",
    "                #Entropy of the signals (Can experiment with different types of Entropy)\n",
    "                entropy_x = ent.shannon_entropy(X_list)\n",
    "                entropy_y = ent.shannon_entropy(Y_list)\n",
    "                entropy_z = ent.shannon_entropy(Z_list)\n",
    "                \n",
    "                \n",
    "                #Interquartile range of the signals\n",
    "                iqr_x = iqr(X_list)\n",
    "                iqr_y = iqr(Y_list)\n",
    "                iqr_z = iqr(Z_list)\n",
    "               \n",
    "    \n",
    "    # ****************** Frequency-Domain Features ************************* #\n",
    "                \n",
    "                #Normalized FFT coefficients\n",
    "                fft_x = LA.norm(np.fft.rfft(X_list))              \n",
    "                fft_y = LA.norm(np.fft.rfft(Y_list))   \n",
    "                fft_z = LA.norm(np.fft.rfft(Z_list))  \n",
    "                \n",
    "                \n",
    "                #Store the features\n",
    "                window_feature = []\n",
    "                window_feature.append(mean_x)\n",
    "                window_feature.append(mean_y)\n",
    "                window_feature.append(mean_z)\n",
    "                \n",
    "                window_feature.append(var_x)\n",
    "                window_feature.append(var_y)\n",
    "                window_feature.append(var_z)\n",
    "                \n",
    "                window_feature.append(median_x)\n",
    "                window_feature.append(median_y)\n",
    "                window_feature.append(median_z)\n",
    "                \n",
    "                window_feature.append(std_x)\n",
    "                window_feature.append(std_y)\n",
    "                window_feature.append(std_z)\n",
    "                \n",
    "                \n",
    "                window_feature.append(signal_mag_area)\n",
    "                \n",
    "                window_feature.append(max_x)\n",
    "                window_feature.append(max_index_x)\n",
    "                window_feature.append(min_x)\n",
    "                window_feature.append(min_index_x)\n",
    "                \n",
    "                window_feature.append(max_y)\n",
    "                window_feature.append(max_index_y)\n",
    "                window_feature.append(min_y)\n",
    "                window_feature.append(min_index_y)\n",
    "                \n",
    "                window_feature.append(max_z)\n",
    "                window_feature.append(max_index_z)\n",
    "                window_feature.append(min_z)\n",
    "                window_feature.append(min_index_z)\n",
    "                \n",
    "                window_feature.append(power_x)\n",
    "                window_feature.append(power_y)\n",
    "                window_feature.append(power_z)\n",
    "                \n",
    "                window_feature.append(skew_x)\n",
    "                window_feature.append(kurtosis_x) \n",
    "                \n",
    "                window_feature.append(skew_y)\n",
    "                window_feature.append(kurtosis_y) \n",
    "                \n",
    "                window_feature.append(skew_z)\n",
    "                window_feature.append(kurtosis_z) \n",
    "                \n",
    "                window_feature.append(entropy_x)\n",
    "                window_feature.append(entropy_y)\n",
    "                window_feature.append(entropy_z)\n",
    "                \n",
    "                window_feature.append(iqr_x)\n",
    "                window_feature.append(iqr_y)\n",
    "                window_feature.append(iqr_z)\n",
    "                \n",
    "                window_feature.append(fft_x)\n",
    "                window_feature.append(fft_y)\n",
    "                window_feature.append(fft_z)\n",
    "                \n",
    "                \n",
    "                #window_feature.append(num_peaks_x)\n",
    "                #window_feature.append(num_peaks_y)\n",
    "                #window_feature.append(num_peaks_z)\n",
    "               \n",
    "                #scale = preprocessing.minmax_scale(data, feature_range=(-0.5, 0.5))\n",
    "\n",
    "                features.append(window_feature)\n",
    "\n",
    "                #Store the label\n",
    "                labels.append(df[5].iloc[1])\n",
    "\n",
    "            dfs_list.append(df)\n",
    "\n",
    "    dfs = pd.concat(dfs_list)\n",
    "\n",
    "    pickle.dump(features, open(\"pickles/accelerometer_features.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(labels, open(\"pickles/accelerometer_labels.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(dfs_list, open(\"pickles/accelerometer_dfs_list.pickle\", \"wb\"), protocol=2)\n",
    "\n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8196, 43)\n",
      "(8196,)\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['laying_down' 'sitting' 'standing' 'walking']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1947.,    0.,    0., 2842.,    0.,    0., 1656.,    0.,    0.,\n",
       "        1751.]),\n",
       " array([0. , 0.3, 0.6, 0.9, 1.2, 1.5, 1.8, 2.1, 2.4, 2.7, 3. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRhkAktSo/wPG+nm2fSsVxAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22e2fdffd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "print(le.classes_)\n",
    "\n",
    "plt.hist(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute features for accelerometer and gyrometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "data_3_4_2018/0_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/0_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (34142, 9)\n",
      "After trimming:  (29143, 9)\n",
      "data_3_4_2018/10_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/10_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (34053, 9)\n",
      "After trimming:  (29054, 9)\n",
      "data_3_4_2018/11_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/11_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (34366, 9)\n",
      "After trimming:  (29367, 9)\n",
      "data_3_4_2018/12_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/12_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (31982, 9)\n",
      "After trimming:  (26983, 9)\n",
      "data_3_4_2018/13_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/13_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (40277, 9)\n",
      "After trimming:  (35278, 9)\n",
      "data_3_4_2018/14_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/14_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (34135, 9)\n",
      "After trimming:  (29136, 9)\n",
      "data_3_4_2018/15_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/15_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (32930, 9)\n",
      "After trimming:  (27931, 9)\n",
      "data_3_4_2018/16_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/16_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (43705, 9)\n",
      "After trimming:  (38706, 9)\n",
      "data_3_4_2018/17_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/17_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (44011, 9)\n",
      "After trimming:  (39012, 9)\n",
      "data_3_4_2018/18_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/18_4_android.sensor.gyroscope.data.csv\n",
      "Before trimming:  (45456, 9)\n",
      "After trimming:  (40457, 9)\n",
      "data_3_4_2018/19_1_android.sensor.accelerometer.data.csv\n",
      "data_3_4_2018/19_4_android.sensor.gyroscope.data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "in_dir = \"data_3_4_2018\"\n",
    "\n",
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "def number_of_peaks(window):\n",
    "    indexes = find_peaks_cwt(window, np.arange(1, len(window)))\n",
    "\n",
    "    return len(indexes)\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def combine_acc_gyro_data(df_acc, df_gyro):\n",
    "    threshold = 10\n",
    "    \n",
    "    acc_index = 0\n",
    "    gyro_index = 0\n",
    "    \n",
    "    acc_matrix = df_acc.as_matrix()\n",
    "    gyro_matrix = df_gyro.as_matrix()\n",
    "    \n",
    "    combined_list = []\n",
    "    \n",
    "    while (acc_index < df_acc.shape[0]) and (gyro_index < df_gyro.shape[0]):\n",
    "        #find next gyro_index within threshold and append the data\n",
    "        \n",
    "        acc_time = df_acc.iloc[acc_index][0]\n",
    "        gyro_time = df_gyro.iloc[gyro_index][0]\n",
    "\n",
    "        if (acc_time < gyro_time):\n",
    "            while (acc_index < df_acc.shape[0]) and (gyro_time - acc_time > threshold):\n",
    "                acc_time = acc_matrix[acc_index][0] #df_acc.iloc[acc_index][0]\n",
    "                acc_index += 1\n",
    "        else:\n",
    "            while (gyro_index < df_gyro.shape[0]) and (acc_time - gyro_time > threshold):\n",
    "                gyro_time = gyro_matrix[gyro_index][0] #df_gyro.iloc[gyro_index][0]\n",
    "                gyro_index += 1\n",
    "            \n",
    "        combined_list.append(np.concatenate((acc_matrix[acc_index][:4], gyro_matrix[gyro_index][1:6]), axis=0))\n",
    "        acc_index += 1\n",
    "        gyro_index += 1\n",
    "            \n",
    "    return combined_list\n",
    "\n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "pickle_file = Path(\"pickles/acc_gyro_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for accelerometer and gyroscope features combined\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/acc_gyro_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/acc_gyro_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/acc_gyro_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            if 'accelerometer' in f:\n",
    "                accelerometer_path = \"/\".join(path) + \"/\" + f\n",
    "                print(accelerometer_path)\n",
    "\n",
    "                first_ = f.find(\"_\")\n",
    "                g = f[0:first_] + \"_4\" + f[first_+2:]\n",
    "                gyroscope_path = \"/\".join(path) + \"/\" + g.replace(\"accelerometer\",\"gyroscope\")\n",
    "                print(gyroscope_path)\n",
    "\n",
    "                df_acc = pd.read_csv(accelerometer_path, header=None)\n",
    "                df_gyro = pd.read_csv(gyroscope_path, header=None)\n",
    "\n",
    "                combined_list = combine_acc_gyro_data(df_acc, df_gyro)\n",
    "                combined_numpy = np.array(combined_list)\n",
    "\n",
    "                combined_df = pd.DataFrame(data=combined_numpy)\n",
    "\n",
    "                print(\"Before trimming: \", combined_df.shape)\n",
    "\n",
    "                combined_df = trim_first_last_n_seconds(combined_df, trim_num_seconds, acc_freq)\n",
    "                if combined_df is None:\n",
    "                    print(\"Continuing\")\n",
    "                    continue\n",
    "\n",
    "                print(\"After trimming: \", combined_df.shape)\n",
    "\n",
    "                #Sample the data according to the size of the window with 50% overlap\n",
    "                for index in range(0, combined_df.shape[0]-window_size, window_jump_steps):\n",
    "                    indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                    window = combined_df.iloc[indexes,:]\n",
    "\n",
    "                    #Generate the features for this window\n",
    "                    acc_mean_x = np.mean(window[1].tolist())\n",
    "                    acc_mean_y = np.mean(window[2].tolist())\n",
    "                    acc_mean_z = np.mean(window[3].tolist())\n",
    "\n",
    "                    acc_var_x = np.var(window[1].tolist())\n",
    "                    acc_var_y = np.var(window[2].tolist())\n",
    "                    acc_var_z = np.var(window[3].tolist())\n",
    "\n",
    "#                     acc_num_peaks_x = number_of_peaks(window[1].tolist())\n",
    "#                     acc_num_peaks_y = number_of_peaks(window[2].tolist())\n",
    "#                     acc_num_peaks_z = number_of_peaks(window[3].tolist())            \n",
    "\n",
    "                    window_feature = []\n",
    "\n",
    "                    window_feature.append(acc_mean_x)\n",
    "                    window_feature.append(acc_mean_y)\n",
    "                    window_feature.append(acc_mean_z)\n",
    "                    window_feature.append(acc_var_x)\n",
    "                    window_feature.append(acc_var_y)\n",
    "                    window_feature.append(acc_var_z)\n",
    "#                     window_feature.append(acc_num_peaks_x)\n",
    "#                     window_feature.append(acc_num_peaks_y)\n",
    "#                     window_feature.append(acc_num_peaks_z)\n",
    "\n",
    "\n",
    "                    gyro_mean_x = np.mean(window[4].tolist())\n",
    "                    gyro_mean_y = np.mean(window[5].tolist())\n",
    "                    gyro_mean_z = np.mean(window[6].tolist())\n",
    "\n",
    "                    gyro_var_x = np.var(window[4].tolist())\n",
    "                    gyro_var_y = np.var(window[5].tolist())\n",
    "                    gyro_var_z = np.var(window[6].tolist())\n",
    "\n",
    "#                     gyro_num_peaks_x = number_of_peaks(window[4].tolist())\n",
    "#                     gyro_num_peaks_y = number_of_peaks(window[5].tolist())\n",
    "#                     gyro_num_peaks_z = number_of_peaks(window[6].tolist())            \n",
    "\n",
    "                    #Store the features\n",
    "                    \n",
    "                    window_feature.append(gyro_mean_x)\n",
    "                    window_feature.append(gyro_mean_y)\n",
    "                    window_feature.append(gyro_mean_z)\n",
    "                    window_feature.append(gyro_var_x)\n",
    "                    window_feature.append(gyro_var_y)\n",
    "                    window_feature.append(gyro_var_z)\n",
    "#                     window_feature.append(gyro_num_peaks_x)\n",
    "#                     window_feature.append(gyro_num_peaks_y)\n",
    "#                     window_feature.append(gyro_num_peaks_z)\n",
    "\n",
    "                    features.append(window_feature)\n",
    "\n",
    "                    #Store the label\n",
    "                    labels.append(combined_df[8].iloc[0])\n",
    "                \n",
    "                dfs_list.append(combined_df)\n",
    "            \n",
    "    dfs = pd.concat(dfs_list)\n",
    "\n",
    "    pickle.dump(features, open(\"pickles/acc_gyro_features.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(labels, open(\"pickles/acc_gyro_labels.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(dfs_list, open(\"pickles/acc_gyro_dfs_list.pickle\", \"wb\"), protocol=2)\n",
    "\n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute LSTM features for accelerometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "data_3_4_2018/0_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64964, 6)\n",
      "After trimming:  (59965, 6)\n",
      "data_3_4_2018/10_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64675, 6)\n",
      "After trimming:  (59676, 6)\n",
      "data_3_4_2018/11_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (66938, 6)\n",
      "After trimming:  (61939, 6)\n",
      "data_3_4_2018/12_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (60980, 6)\n",
      "After trimming:  (55981, 6)\n",
      "data_3_4_2018/13_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (62506, 6)\n",
      "After trimming:  (57507, 6)\n",
      "data_3_4_2018/14_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65108, 6)\n",
      "After trimming:  (60109, 6)\n",
      "data_3_4_2018/15_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (62588, 6)\n",
      "After trimming:  (57589, 6)\n",
      "data_3_4_2018/16_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (82181, 6)\n",
      "After trimming:  (77182, 6)\n",
      "data_3_4_2018/17_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (83784, 6)\n",
      "After trimming:  (78785, 6)\n",
      "data_3_4_2018/18_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (85978, 6)\n",
      "After trimming:  (80979, 6)\n",
      "data_3_4_2018/19_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (101168, 6)\n",
      "After trimming:  (96169, 6)\n",
      "data_3_4_2018/1_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (50274, 6)\n",
      "After trimming:  (45275, 6)\n",
      "data_3_4_2018/20_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (53812, 6)\n",
      "After trimming:  (48813, 6)\n",
      "data_3_4_2018/21_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65720, 6)\n",
      "After trimming:  (60721, 6)\n",
      "data_3_4_2018/22_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (71074, 6)\n",
      "After trimming:  (66075, 6)\n",
      "data_3_4_2018/23_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (30053, 6)\n",
      "After trimming:  (25054, 6)\n",
      "data_3_4_2018/24_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (26070, 6)\n",
      "After trimming:  (21071, 6)\n",
      "data_3_4_2018/25_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (718108, 6)\n",
      "After trimming:  (713109, 6)\n",
      "data_3_4_2018/26_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (66885, 6)\n",
      "After trimming:  (61886, 6)\n",
      "data_3_4_2018/27_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (68901, 6)\n",
      "After trimming:  (63902, 6)\n",
      "data_3_4_2018/28_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (69512, 6)\n",
      "After trimming:  (64513, 6)\n",
      "data_3_4_2018/29_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (67293, 6)\n",
      "After trimming:  (62294, 6)\n",
      "data_3_4_2018/2_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (59060, 6)\n",
      "After trimming:  (54061, 6)\n",
      "data_3_4_2018/30_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (67641, 6)\n",
      "After trimming:  (62642, 6)\n",
      "data_3_4_2018/31_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (40200, 6)\n",
      "After trimming:  (35201, 6)\n",
      "data_3_4_2018/32_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (118805, 6)\n",
      "After trimming:  (113806, 6)\n",
      "data_3_4_2018/33_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65634, 6)\n",
      "After trimming:  (60635, 6)\n",
      "data_3_4_2018/34_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (98968, 6)\n",
      "After trimming:  (93969, 6)\n",
      "data_3_4_2018/35_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (96365, 6)\n",
      "After trimming:  (91366, 6)\n",
      "data_3_4_2018/36_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (2323, 6)\n",
      "Continuing\n",
      "data_3_4_2018/37_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (43060, 6)\n",
      "After trimming:  (38061, 6)\n",
      "data_3_4_2018/38_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (83999, 6)\n",
      "After trimming:  (79000, 6)\n",
      "data_3_4_2018/39_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (65899, 6)\n",
      "After trimming:  (60900, 6)\n",
      "data_3_4_2018/3_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (55316, 6)\n",
      "After trimming:  (50317, 6)\n",
      "data_3_4_2018/40_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (89512, 6)\n",
      "After trimming:  (84513, 6)\n",
      "data_3_4_2018/41_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (59739, 6)\n",
      "After trimming:  (54740, 6)\n",
      "data_3_4_2018/42_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (57828, 6)\n",
      "After trimming:  (52829, 6)\n",
      "data_3_4_2018/43_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (35267, 6)\n",
      "After trimming:  (30268, 6)\n",
      "data_3_4_2018/44_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (52974, 6)\n",
      "After trimming:  (47975, 6)\n",
      "data_3_4_2018/4_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (48531, 6)\n",
      "After trimming:  (43532, 6)\n",
      "data_3_4_2018/5_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (63094, 6)\n",
      "After trimming:  (58095, 6)\n",
      "data_3_4_2018/6_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (63971, 6)\n",
      "After trimming:  (58972, 6)\n",
      "data_3_4_2018/7_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64212, 6)\n",
      "After trimming:  (59213, 6)\n",
      "data_3_4_2018/8_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (64175, 6)\n",
      "After trimming:  (59176, 6)\n",
      "data_3_4_2018/9_1_android.sensor.accelerometer.data.csv\n",
      "Before trimming:  (52095, 6)\n",
      "After trimming:  (47096, 6)\n"
     ]
    }
   ],
   "source": [
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "pickle_file = Path(\"pickles/lstm_acc_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for LSTM acc and gyro\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/lstm_acc_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/lstm_acc_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/lstm_acc_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            accelerometer_path = \"/\".join(path) + \"/\" + f\n",
    "            print(accelerometer_path)\n",
    "\n",
    "            first_ = f.find(\"_\")\n",
    "            g = f[0:first_] + \"_4\" + f[first_+2:]\n",
    "            gyroscope_path = \"/\".join(path) + \"/\" + g.replace(\"accelerometer\",\"gyroscope\")\n",
    "            print(gyroscope_path)\n",
    "\n",
    "            df_acc = pd.read_csv(accelerometer_path, header=None)\n",
    "            df_gyro = pd.read_csv(gyroscope_path, header=None)\n",
    "\n",
    "            combined_list = combine_acc_gyro_data(df_acc, df_gyro)\n",
    "            combined_numpy = np.array(combined_list)\n",
    "\n",
    "            combined_df = pd.DataFrame(data=combined_numpy)\n",
    "\n",
    "            print(\"Before trimming: \", combined_df.shape)\n",
    "\n",
    "            combined_df = trim_first_last_n_seconds(combined_df, trim_num_seconds, acc_freq)\n",
    "            if combined_df is None:\n",
    "                print(\"Continuing\")\n",
    "                continue\n",
    "\n",
    "            print(\"After trimming: \", combined_df.shape)\n",
    "\n",
    "            #Sample the data according to the size of the window with 50% overlap\n",
    "            for index in range(0, combined_df.shape[0]-window_size, window_jump_steps):\n",
    "                indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                window = df.iloc[indexes, 1:df.shape[1]-2]\n",
    "\n",
    "                #Generate the features for this window            \n",
    "                features.append(np.asarray(window))\n",
    "\n",
    "                #Store the label\n",
    "                labels.append(df[5].iloc[0])\n",
    "\n",
    "            dfs_list.append(df)\n",
    "\n",
    "    dfs = pd.concat(dfs_list)\n",
    "    \n",
    "    pickle.dump(features, open(\"pickles/lstm_acc_features.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(labels, open(\"pickles/lstm_acc_labels.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(dfs_list, open(\"pickles/lstm_acc_dfs_list.pickle\", \"wb\"), protocol=2)\n",
    "    \n",
    "    \n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute LSTM features for accelerometer and gyroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window_size, Window_jump_steps:  1000 500\n",
      "Found pickle files for LSTM acc and gyro\n"
     ]
    }
   ],
   "source": [
    "trim_num_seconds = 10\n",
    "acc_freq = 4\n",
    "window_num_seconds = 4 #seconds\n",
    "steps_per_sec = int(1000/acc_freq)\n",
    "window_size = int(window_num_seconds*steps_per_sec)\n",
    "window_step = 2 #seconds\n",
    "window_jump_steps = int(window_step*steps_per_sec)\n",
    "\n",
    "print(\"Window_size, Window_jump_steps: \", window_size, window_jump_steps)\n",
    "\n",
    "dfs_list = []\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "#this function assumes that records are evenly spaced\n",
    "def trim_first_last_n_seconds(df, n, freq):\n",
    "    if df.shape[0] < 6001:\n",
    "        return None\n",
    "    \n",
    "    remove_indexes = list(range(0, int(n*1000/freq)))\n",
    "    df = df.drop(remove_indexes)\n",
    "\n",
    "    remove_indexes = list(range(df.shape[0] - int(n*1000/freq), df.shape[0]-1))\n",
    "    df = df.drop(remove_indexes)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def combine_acc_gyro_data(df_acc, df_gyro):\n",
    "    threshold = 10\n",
    "    \n",
    "    acc_index = 0\n",
    "    gyro_index = 0\n",
    "    \n",
    "    acc_matrix = df_acc.as_matrix()\n",
    "    gyro_matrix = df_gyro.as_matrix()\n",
    "    \n",
    "    combined_list = []\n",
    "    \n",
    "    while (acc_index < df_acc.shape[0]) and (gyro_index < df_gyro.shape[0]):\n",
    "        #find next gyro_index within threshold and append the data\n",
    "        \n",
    "        acc_time = df_acc.iloc[acc_index][0]\n",
    "        gyro_time = df_gyro.iloc[gyro_index][0]\n",
    "\n",
    "        if (acc_time < gyro_time):\n",
    "            while (acc_index < df_acc.shape[0]) and (gyro_time - acc_time > threshold):\n",
    "                acc_time = acc_matrix[acc_index][0] #df_acc.iloc[acc_index][0]\n",
    "                acc_index += 1\n",
    "        else:\n",
    "            while (gyro_index < df_gyro.shape[0]) and (acc_time - gyro_time > threshold):\n",
    "                gyro_time = gyro_matrix[gyro_index][0] #df_gyro.iloc[gyro_index][0]\n",
    "                gyro_index += 1\n",
    "            \n",
    "        combined_list.append(np.concatenate((acc_matrix[acc_index][:4], gyro_matrix[gyro_index][1:6]), axis=0))\n",
    "        acc_index += 1\n",
    "        gyro_index += 1\n",
    "            \n",
    "    return combined_list\n",
    "\n",
    "\n",
    "pickle_file = Path(\"pickles/lstm_acc_gyro_features.pickle\")\n",
    "\n",
    "if pickle_file.exists():\n",
    "    print(\"Found pickle files for LSTM acc and gyro\")\n",
    "    \n",
    "    features = pickle.load(open(\"pickles/lstm_acc_gyro_features.pickle\", \"rb\"))\n",
    "    labels = pickle.load(open(\"pickles/lstm_acc_gyro_labels.pickle\", \"rb\"))\n",
    "    dfs_list = pickle.load(open(\"pickles/lstm_acc_gyro_dfs_list.pickle\", \"rb\"))\n",
    "    \n",
    "else:\n",
    "\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        path = root.split(os.sep)\n",
    "\n",
    "        for f in files:\n",
    "\n",
    "            if 'accelerometer' in f:\n",
    "                accelerometer_path = \"/\".join(path) + \"/\" + f\n",
    "                print(accelerometer_path)\n",
    "\n",
    "                first_ = f.find(\"_\")\n",
    "                g = f[0:first_] + \"_4\" + f[first_+2:]\n",
    "                gyroscope_path = \"/\".join(path) + \"/\" + g.replace(\"accelerometer\",\"gyroscope\")\n",
    "                print(gyroscope_path)\n",
    "\n",
    "                df_acc = pd.read_csv(accelerometer_path, header=None)\n",
    "                df_gyro = pd.read_csv(gyroscope_path, header=None)\n",
    "\n",
    "                combined_list = combine_acc_gyro_data(df_acc, df_gyro)\n",
    "                combined_numpy = np.array(combined_list)\n",
    "\n",
    "                combined_df = pd.DataFrame(data=combined_numpy)\n",
    "\n",
    "                print(\"Before trimming: \", combined_df.shape)\n",
    "\n",
    "                combined_df = trim_first_last_n_seconds(combined_df, trim_num_seconds, acc_freq)\n",
    "                if combined_df is None:\n",
    "                    print(\"Continuing\")\n",
    "                    continue\n",
    "\n",
    "                print(\"After trimming: \", combined_df.shape)\n",
    "\n",
    "                #Sample the data according to the size of the window with 50% overlap\n",
    "                for index in range(0, combined_df.shape[0]-window_size, window_jump_steps):\n",
    "                    indexes = list(range(index, index + int(window_size)))\n",
    "\n",
    "                    window = combined_df.iloc[indexes, 1:7] #,1:combined_df.shape[1]-2]\n",
    "\n",
    "                    #Generate the features for this window            \n",
    "                    features.append(np.asarray(window))\n",
    "\n",
    "                    #Store the label\n",
    "                    labels.append(combined_df[8].iloc[0])\n",
    "\n",
    "                dfs_list.append(combined_df)\n",
    "\n",
    "    dfs = pd.concat(dfs_list)\n",
    "    \n",
    "    pickle.dump(features, open(\"pickles/lstm_acc_gyro_features.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(labels, open(\"pickles/lstm_acc_gyro_labels.pickle\", \"wb\"), protocol=2)\n",
    "    pickle.dump(dfs_list, open(\"pickles/lstm_acc_gyro_dfs_list.pickle\", \"wb\"), protocol=2)\n",
    "    \n",
    "    \n",
    "features = np.asarray(features)\n",
    "labels = np.asarray(labels)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel(modelName, args):\n",
    "    \n",
    "    if modelName == 'LogisticRegression':\n",
    "        model = LogisticRegression(random_state=42)\n",
    "        \n",
    "    if modelName == 'SVC':\n",
    "        model = SVC(random_state=42, kernel=args[0], C=args[1], decision_function_shape=args[2])\n",
    "        \n",
    "    if modelName == 'DecisionTreeClassifier':\n",
    "        model = DecisionTreeClassifier(random_state=42, max_features=args[0], criterion=args[1])\n",
    "        \n",
    "    if modelName == 'RandomForestClassifier':\n",
    "        model = RandomForestClassifier(n_estimators=args[0], criterion=args[1], max_features=args[2], max_depth=args[3], oob_score=True, random_state=42)\n",
    "        \n",
    "    if modelName == 'MLPClassifier':\n",
    "        model = MLPClassifier(hidden_layer_sizes=args[0], activation=args[1], solver=args[2], random_state=42, max_iter=500)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_kfold(features, labels, num_splits, modelName, args=None, verbose=False):\n",
    "    \n",
    "    X = np.array(normalize(features))\n",
    "    y = np.array(labels)\n",
    "\n",
    "    kf = KFold(n_splits=num_splits, random_state=None, shuffle=True)\n",
    "    kf.get_n_splits(X)\n",
    "\n",
    "    foldAccuracy = list()\n",
    "    foldPrecision = list()\n",
    "    foldRecall = list()\n",
    "    bestModel = None\n",
    "    bestAccuracy = float(sys.maxsize) * (-1)\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = getModel(modelName, args)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        prediction = model.predict(X_test)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"\\nFold: \", fold)\n",
    "            print(\"Confusion Matrix:\")\n",
    "            cm = metrics.confusion_matrix(yTest, prediction)\n",
    "            print(cm)\n",
    "            plt.matshow(cm, cmap = plt.cm.Oranges)\n",
    "            plt.title('Confusion matrix')\n",
    "            plt.colorbar()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            plt.show()\n",
    "\n",
    "        accuracy = metrics.accuracy_score(y_test, prediction)\n",
    "        precision = metrics.precision_score(y_test, prediction, average = None)\n",
    "        recall = metrics.recall_score(y_test, prediction, average = None)\n",
    "\n",
    "        foldAccuracy.append(accuracy)\n",
    "        foldPrecision.append(precision)\n",
    "        foldRecall.append(recall)\n",
    "        \n",
    "        if accuracy > bestAccuracy:\n",
    "            bestAccuracy = accuracy\n",
    "            bestModel = model\n",
    "                \n",
    "    print(\"\\nBest Accuracy: \", bestAccuracy)\n",
    "    \n",
    "    return bestModel, foldAccuracy, foldPrecision, foldRecall        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter data to select some specific features from all the list of features for the model training\n",
    "\n",
    "features_to_consider = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "features_new = []\n",
    "\n",
    "for feature in features:\n",
    "    \n",
    "    feat = []\n",
    "    for index in features_to_consider:\n",
    "        feat.append(feature[index])\n",
    "        \n",
    "    features_new.append(feat)\n",
    "\n",
    "features_new = numpy.array(features_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.67587902e+00,  2.17294647e-02,  9.13332639e+00,  7.17382979e-04,\n",
       "        1.91591425e-03,  7.78390033e-04, -2.67608980e+00,  2.39364020e-02,\n",
       "        9.12934300e+00,  2.67840060e-02,  4.37711578e-02,  2.78996422e-02])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_new[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.7882855399633923\n"
     ]
    }
   ],
   "source": [
    "num_splits = 5\n",
    "\n",
    "lrModel, lrAccuracy, lrPrecision, lrRecall = run_kfold(features_new, labels, num_splits, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8547895057962172\n"
     ]
    }
   ],
   "source": [
    "#hard margin SVM 1\n",
    "\n",
    "#kernel = 'rbf'\n",
    "#C = 1000\n",
    "#decision_function_shape = 'ovo'\n",
    "\n",
    "num_splits = 5\n",
    "args = ['rbf', 1000, 'ovo']\n",
    "\n",
    "hmSVC1Model, hmSVC1Accuracy, hmSVC1Precision, hmSVC1Recall = run_kfold(features_new, labels, num_splits, 'SVC', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8871262965222697\n"
     ]
    }
   ],
   "source": [
    "#decision Tree 1\n",
    "\n",
    "#max_features = 6\n",
    "#criterion = 'gini'\n",
    "\n",
    "num_splits = 5\n",
    "args = [6, 'gini']\n",
    "\n",
    "dt1Model, dt1Accuracy, dt1Precision, dt1Recall = run_kfold(features_new, labels, num_splits, 'DecisionTreeClassifier', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.8822452715070165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:451: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
      "  warn(\"Some inputs do not have OOB scores. \"\n",
      "c:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:456: RuntimeWarning: invalid value encountered in true_divide\n",
      "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
     ]
    }
   ],
   "source": [
    "#random Forest Classification 1\n",
    "\n",
    "#n_estimators = 20\n",
    "#criterion = gini\n",
    "#max_features = 6\n",
    "#max_depth = 5\n",
    "\n",
    "num_splits = 5\n",
    "args = [20, 'gini', 6, 8]\n",
    "\n",
    "rf1Model, rf1Accuracy, rf1Precision, rf1Recall = run_kfold(features_new, labels, num_splits, 'RandomForestClassifier', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Accuracy:  0.9243441122635754\n"
     ]
    }
   ],
   "source": [
    "#Multi Layer Perceptron Classification \n",
    "\n",
    "#hidden_layer_sizes = 50\n",
    "#activation = tanh\n",
    "#solver = lbfgs\n",
    "\n",
    "num_splits = 5\n",
    "args = [(50,100), 'tanh', 'lbfgs']\n",
    "\n",
    "mlp2Model, mlp2Accuracy, mlp2Precision, mlp2Recall = run_kfold(features_new, labels, num_splits, 'MLPClassifier', args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_lstm(features, labels, train_test_split_ratio, batch_size, epochs):\n",
    "\n",
    "    X = features\n",
    "\n",
    "    y = np.array(labels)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    y = y.reshape(-1,1)\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    y = ohe.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=train_test_split_ratio, random_state=42)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(X_train[0].shape[0], X_train[0].shape[1]), return_sequences=True))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "    #Evaluate the model\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    predicted_labels = model.predict(X_test)\n",
    "\n",
    "    print(\"Accuracy: \", scores[1]*100)\n",
    "\n",
    "    def get_hot_value(my_list):\n",
    "        max_val = max(my_list)\n",
    "        return [int(item == max_val) for item in my_list]\n",
    "\n",
    "    hot_list = [get_hot_value(sublist) for sublist in predicted_labels]\n",
    "\n",
    "    t  = np.array(y_test)\n",
    "    p  = np.array(hot_list)\n",
    "    t = np.argmax(t, axis=1)\n",
    "    p = np.argmax(p, axis=1)\n",
    "\n",
    "    print(\"Precision: \", precision_score(t, p, average=None))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1000, 100)         42800     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 123,604\n",
      "Trainable params: 123,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      " 600/3808 [===>..........................] - ETA: 1:39 - loss: 1.4339 - acc: 0.2633"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-19b097efe5e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mrun_lstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_split_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-4e81fe15c768>\u001b[0m in \u001b[0;36mrun_lstm\u001b[1;34m(features, labels, train_test_split_ratio, batch_size, epochs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m#Evaluate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1649\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1651\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2350\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2351\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2352\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sharm\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_test_split_ratio = 0.1\n",
    "batch_size = 300\n",
    "epochs = 100\n",
    "\n",
    "run_lstm(features, labels, train_test_split_ratio, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
